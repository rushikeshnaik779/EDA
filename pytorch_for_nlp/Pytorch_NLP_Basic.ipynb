{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Pytorch NLP Basic.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyOqbaqCq8BMoGsz//KePchh",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rushikeshnaik779/EDA/blob/master/pytorch_for_nlp/Pytorch_NLP_Basic.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oigLGBBgM2JP",
        "outputId": "acf0e676-1831-4253-be52-289ce0e4aa3f"
      },
      "source": [
        "import spacy \n",
        "nlp = spacy.load('en')\n",
        "text = \"Marry, don't slap the green witch\"\n",
        "print([str(token) for token in nlp(text.lower())])"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['marry', ',', 'do', \"n't\", 'slap', 'the', 'green', 'witch']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A_sc05gENSfu",
        "outputId": "9e2fb794-bc8e-4ea3-da8e-6b7d35d264e1"
      },
      "source": [
        "from nltk.tokenize import TweetTokenizer \n",
        "tweet=u\"Snow White and the Seven Degrees #MakeAMovieCold@midnight :­-)\" \n",
        "tokenizer = TweetTokenizer() \n",
        "print(tokenizer.tokenize(tweet.lower()))"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['snow', 'white', 'and', 'the', 'seven', 'degrees', '#makeamoviecold', '@midnight', ':', '\\xad', '-', ')']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "osGg3nv3OTaE"
      },
      "source": [
        "# Generating N-grams \n",
        "\n",
        "def n_grams(text, n=1):\n",
        "    \"\"\"\n",
        "    takes tokens or text, returns a list of n-grams \n",
        "    \"\"\"\n",
        "    return [text[i: i+n] for i in range(len(text)-n+1)]"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D0US69v0PfPh",
        "outputId": "71d9692f-1b61-47c1-fd16-b5d88cb2f3a3"
      },
      "source": [
        "cleaned = ['mary', ',', \"n't\", 'slap', 'green', 'witch', '.'] \n",
        "print(n_grams(cleaned, 5))"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[['mary', ',', \"n't\", 'slap', 'green'], [',', \"n't\", 'slap', 'green', 'witch'], [\"n't\", 'slap', 'green', 'witch', '.']]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rHWMxUXfPjFK",
        "outputId": "827cdcc7-9b8b-4775-b0b9-137051b39d20"
      },
      "source": [
        "# Lemmatization: reducing words to their root forms\n",
        "\n",
        "import spacy \n",
        "nlp = spacy.load(\"en\")\n",
        "doc = nlp(u\"he was running late\")\n",
        "for token in doc: \n",
        "    print('{} --> {}'.format(token, token.lemma_))"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "he --> -PRON-\n",
            "was --> be\n",
            "running --> run\n",
            "late --> late\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cHYaRCwZRovj"
      },
      "source": [
        "spaCy, for example, uses a predefined dictionary, called WordNet, for extracting lemmas, but lemmatization can be framed as a machine learning problem requiring an understanding of the morphology of the language."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l3WpyARCRKgF"
      },
      "source": [
        "# CATEGORIZATION SENTENCES AND DOCUMENTS "
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H6noyrPTSZam"
      },
      "source": [
        "Categorizing or classifying documents is probably one of the earliest applications of NLP. The TF and TF­IDF representations we described in hapter 1 are immediately useful for classifying and categorizing longer chunks of text such as documents or sentences. Problems such as assigning topic labels, predicting sentiment of reviews, filtering spam emails, language identification, and email triaging can be framed as supervised document classification problems. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MLoohcGSSCCb",
        "outputId": "278027d0-2fb0-4a37-f0df-7e4a8fccc225"
      },
      "source": [
        "# Categorizing Words: POS Tagging\n",
        "\n",
        "import spacy \n",
        "nlp = spacy.load('en')\n",
        "\n",
        "doc = nlp(u\"Marry slapped the green witch . !! !!!! ++ ++\")\n",
        "for token in doc: \n",
        "    print(\"{} --> {}\".format(token, token.pos_))"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Marry --> PROPN\n",
            "slapped --> VERB\n",
            "the --> DET\n",
            "green --> ADJ\n",
            "witch --> NOUN\n",
            ". --> PUNCT\n",
            "! --> PUNCT\n",
            "! --> PUNCT\n",
            "! --> PUNCT\n",
            "! --> PUNCT\n",
            "! --> PUNCT\n",
            "! --> PUNCT\n",
            "+ --> CCONJ\n",
            "+ --> NOUN\n",
            "+ --> NOUN\n",
            "+ --> NOUN\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y7Q-N-KsS8LV"
      },
      "source": [
        "# Categorizing Spans: Chunking and Named Entity Recognition\n",
        "\n",
        "import spacy \n",
        "nlp = spacy.load('en')\n",
        "doc = nlp(u\"his is called chunking or shallow parsing. Shallow parsing aims to derive higher­order units composed of the grammatical atoms, like nouns, verbs, adjectives, and so on. It is possible to write regular expressions over the part­of­speech tags to approximate shallow parsing if you do not have data to train models for shallow parsing. Fortunately,\")"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3gO2WUvcUgwV",
        "outputId": "7e8602ea-d6f5-494d-f077-156b9c9b16ea"
      },
      "source": [
        "for chunk in doc.noun_chunks:\n",
        "    print ('{} ­ {}'.format(chunk, chunk.label_))"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "higher­order units ­ NP\n",
            "the grammatical atoms ­ NP\n",
            "nouns ­ NP\n",
            "verbs ­ NP\n",
            "adjectives ­ NP\n",
            "It ­ NP\n",
            "regular expressions ­ NP\n",
            "the part­of­speech tags ­ NP\n",
            "you ­ NP\n",
            "data ­ NP\n",
            "models ­ NP\n",
            "shallow parsing ­ NP\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HwqYqFL0Uyg8"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}